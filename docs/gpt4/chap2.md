---
sidebar_position: 1
---

# 训练

在预训练过程中，GPT-4的训练数据包含了大量网络上爬下来的数据，包括了很多有正解和错误解的数学问题、强推理、弱推理、自相矛盾的，保持一致的陈述、各种各样的意识形态和想法的文本。

预训练好的模型有些是在错误的答案上训练过的，所以刚训练好的模型有些回答并不是我们想要的，为了和人类的意图保持一致并且回答安全可控，使用了基于人类反馈的强化学习（RLHF）来对模型进行微调来教模型如何理解人类的输入和生成对应的输出。

虽然有微调的过程，但是OpenAI的论文指出，RLHF并不能提高模型在考试上的表现，如果调参不妥当甚至还会降低它的能力，因此可以推断出模型强大的文本能力是靠巨量数据、大算例堆起来（力大飞砖）。 而人类的干预是去引导、控制它来正确展示自己的能力，用人类喜欢的方式来回答人类的问题。
